{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Silence Cutter (Notebook)\n",
        "\n",
        "This notebook bundles the whole project in one place. Run each cell top to bottom.\n",
        "\n",
        "Requirements:\n",
        "- `pip install google-genai`\n",
        "- ffmpeg on PATH if you want to render edited video\n",
        "\n",
        "API key:\n",
        "- Set `GEMINI_API_KEY` env var, or fill in the variable in the next cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Definition & Objective\n",
        "This project addresses a common editing task: removing long or awkward pauses from spoken video while keeping pauses that add meaning. The objective is to automate a first-pass trim using only the transcript timestamps, then apply a small LLM decision step to avoid cutting meaningful pauses.\n",
        "\n",
        "Inputs:\n",
        "- A video file (for optional rendering).\n",
        "- A transcript with timestamps (SRT, VTT, or simple start/end lines).\n",
        "\n",
        "Outputs:\n",
        "- A cut plan (JSON) describing all candidate gaps and decisions.\n",
        "- A CSV of keep segments.\n",
        "- An optional edited video created from those keep segments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selected project track\n",
        "Applied AI tooling for media editing. The system uses deterministic time-gap rules for candidate detection and a compact LLM prompt for subjective decisions. This keeps the workflow explainable, easy to test, and fast enough for real editing tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clear problem statement\n",
        "Given a video and a timestamped transcript, detect pauses between captions, decide whether each pause should be CUT or KEEP based on nearby transcript context, and produce an edit plan and edited output.\n",
        "\n",
        "Success criteria:\n",
        "- Gaps above a configurable threshold are evaluated.\n",
        "- Decisions are returned in strict JSON for traceability.\n",
        "- Keep segments are computed and can be rendered into a final video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-world relevance and motivation\n",
        "Editors of lectures, tutorials, interviews, and internal training content often spend time trimming dead air. This is repetitive work that slows turnaround. Automating the first pass reduces manual effort while still allowing a human to review decisions, saving time without removing editorial control.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Understanding & Preparation\n",
        "The transcript is the primary data source. Supported formats:\n",
        "- SRT: numbered blocks with HH:MM:SS,ms timestamps.\n",
        "- VTT: similar to SRT with HH:MM:SS.ms timestamps.\n",
        "- Plain: `start end text` per line.\n",
        "\n",
        "Preparation steps:\n",
        "1) Parse timestamps into seconds.\n",
        "2) Extract caption entries `{start_sec, end_sec, text}`.\n",
        "3) Sort entries in time order.\n",
        "4) Compute gaps between consecutive captions.\n",
        "5) Build context windows of N captions before and after each gap.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model / System Design\n",
        "Hybrid pipeline with two stages:\n",
        "\n",
        "1) Deterministic gap detection\n",
        "- gap = next.start_sec - current.end_sec\n",
        "- if gap >= MIN_GAP_TO_CONSIDER, mark as candidate\n",
        "\n",
        "2) LLM decision step (Gemini)\n",
        "- Provide gap duration and nearby transcript context.\n",
        "- Require strict JSON output with decision and reason.\n",
        "- Validate JSON and retry on failure.\n",
        "- Batch up to 10 candidates per request.\n",
        "\n",
        "This keeps the system controllable (via thresholds and context size) while leveraging the LLM for subjective judgments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Implementation\n",
        "Key components implemented in the notebook and project:\n",
        "\n",
        "- Transcript parsing: detect format, normalize timestamps, and extract caption text.\n",
        "- Gap detection: build candidate gaps with context_before and context_after.\n",
        "- Gemini wrapper: call `gemini-3-flash-preview`, parse and validate JSON.\n",
        "- Cut plan: remove CUT gaps and merge keep segments.\n",
        "- Rendering: optional ffmpeg trim/concat for final video output.\n",
        "\n",
        "Configurable filters:\n",
        "- `min_gap`: smallest pause to evaluate.\n",
        "- `context`: number of captions before and after each gap.\n",
        "- `batch_size`: number of candidates per Gemini request.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation & Analysis\n",
        "Evaluation focuses on practical edit quality:\n",
        "\n",
        "- Quantitative metrics: number of gaps found, CUT vs KEEP counts, and duration reduction.\n",
        "- Qualitative review: spot-check edited segments for pacing and meaning.\n",
        "\n",
        "Typical adjustments:\n",
        "- Lower min_gap for more aggressive trimming.\n",
        "- Increase context to reduce over-cutting meaningful pauses.\n",
        "- Merge short keep segments to avoid awkward micro-clips.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ethical Considerations & Responsible AI\n",
        "- Privacy: only transcript snippets are sent to Gemini; video stays local.\n",
        "- Consent: avoid uploading private or sensitive transcripts without permission.\n",
        "- Transparency: decisions and reasons are logged in JSON for review.\n",
        "- Human oversight: treat edits as a first pass and confirm context-sensitive cuts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion & Future Scope\n",
        "This notebook demonstrates an end-to-end silence cutter that combines deterministic timing rules with LLM context judgments. It produces a clear audit trail and optional rendered output.\n",
        "\n",
        "Future improvements:\n",
        "- Combine transcript gaps with audio-level silence detection.\n",
        "- Add word-level timestamps for finer trimming.\n",
        "- Provide editor exports (EDL/XML) for NLEs.\n",
        "- Add a review UI to override decisions before rendering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "import shutil\n",
        "import subprocess\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Optional: set this if you do not want to use environment variables.\n",
        "GEMINI_API_KEY = \"\"\n",
        "\n",
        "def load_api_key():\n",
        "    key = os.environ.get(\"GEMINI_API_KEY\", \"\").strip()\n",
        "    if not key:\n",
        "        key = GEMINI_API_KEY.strip()\n",
        "    if not key:\n",
        "        raise RuntimeError(\"Missing GEMINI_API_KEY. Set env var or fill GEMINI_API_KEY variable.\")\n",
        "    return key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transcript parsing (SRT/VTT/plain)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TIME_LINE_RE = re.compile(\n",
        "    r\"(?P<start>\\d{2}:\\d{2}:\\d{2}[.,]\\d{3})\\s*-->\\s*(?P<end>\\d{2}:\\d{2}:\\d{2}[.,]\\d{3})\"\n",
        ")\n",
        "\n",
        "\n",
        "def parse_timestamp(ts: str) -> float:\n",
        "    ts = ts.replace(\",\", \".\")\n",
        "    parts = ts.split(\":\")\n",
        "    if len(parts) != 3:\n",
        "        raise ValueError(f\"Invalid timestamp: {ts}\")\n",
        "    hours = int(parts[0])\n",
        "    minutes = int(parts[1])\n",
        "    seconds = float(parts[2])\n",
        "    return hours * 3600 + minutes * 60 + seconds\n",
        "\n",
        "\n",
        "def parse_time_token(token: str) -> float:\n",
        "    token = token.strip()\n",
        "    if \":\" in token:\n",
        "        return parse_timestamp(token)\n",
        "    return float(token)\n",
        "\n",
        "\n",
        "def detect_format(lines: List[str]) -> str:\n",
        "    for line in lines:\n",
        "        if line.strip().upper() == \"WEBVTT\":\n",
        "            return \"vtt\"\n",
        "    for line in lines:\n",
        "        if TIME_LINE_RE.search(line):\n",
        "            return \"srt_vtt\"\n",
        "    return \"plain\"\n",
        "\n",
        "\n",
        "def parse_srt_vtt(lines: List[str]) -> List[Dict[str, object]]:\n",
        "    entries = []\n",
        "    i = 0\n",
        "    total = len(lines)\n",
        "    while i < total:\n",
        "        line = lines[i].strip()\n",
        "        if not line:\n",
        "            i += 1\n",
        "            continue\n",
        "        if line.isdigit() and i + 1 < total and \"-->\" in lines[i + 1]:\n",
        "            i += 1\n",
        "            line = lines[i].strip()\n",
        "        match = TIME_LINE_RE.match(line)\n",
        "        if match:\n",
        "            start = parse_timestamp(match.group(\"start\"))\n",
        "            end = parse_timestamp(match.group(\"end\"))\n",
        "            i += 1\n",
        "            text_lines = []\n",
        "            while i < total and lines[i].strip():\n",
        "                text_lines.append(lines[i].strip())\n",
        "                i += 1\n",
        "            text = \" \".join(text_lines).strip()\n",
        "            entries.append({\"start_sec\": start, \"end_sec\": end, \"text\": text})\n",
        "        else:\n",
        "            i += 1\n",
        "    return entries\n",
        "\n",
        "\n",
        "def parse_plain(lines: List[str]) -> List[Dict[str, object]]:\n",
        "    entries = []\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        if not stripped:\n",
        "            continue\n",
        "        parts = stripped.split()\n",
        "        if len(parts) < 3:\n",
        "            continue\n",
        "        try:\n",
        "            start = parse_time_token(parts[0])\n",
        "            end = parse_time_token(parts[1])\n",
        "        except ValueError:\n",
        "            continue\n",
        "        text = \" \".join(parts[2:]).strip()\n",
        "        entries.append({\"start_sec\": start, \"end_sec\": end, \"text\": text})\n",
        "    return entries\n",
        "\n",
        "\n",
        "def parse_transcript(path: str) -> List[Dict[str, object]]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as handle:\n",
        "        content = handle.read()\n",
        "    lines = content.splitlines()\n",
        "    fmt = detect_format(lines)\n",
        "    if fmt in (\"vtt\", \"srt_vtt\"):\n",
        "        entries = parse_srt_vtt(lines)\n",
        "    else:\n",
        "        entries = parse_plain(lines)\n",
        "    entries.sort(key=lambda item: (item[\"start_sec\"], item[\"end_sec\"]))\n",
        "    return entries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gap detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_gaps(captions: List[Dict[str, object]], min_gap: float = 0.8, context: int = 2):\n",
        "    candidates = []\n",
        "    if not captions:\n",
        "        return candidates\n",
        "    for i in range(len(captions) - 1):\n",
        "        current = captions[i]\n",
        "        next_cap = captions[i + 1]\n",
        "        gap = float(next_cap[\"start_sec\"]) - float(current[\"end_sec\"])\n",
        "        if gap >= min_gap:\n",
        "            before_start = max(0, i - context + 1)\n",
        "            after_end = min(len(captions), i + 1 + context)\n",
        "            context_before = captions[before_start : i + 1]\n",
        "            context_after = captions[i + 1 : after_end]\n",
        "            candidates.append(\n",
        "                {\n",
        "                    \"id\": f\"gap_{i}\",\n",
        "                    \"gap_start\": float(current[\"end_sec\"]),\n",
        "                    \"gap_end\": float(next_cap[\"start_sec\"]),\n",
        "                    \"gap_duration\": gap,\n",
        "                    \"context_before\": context_before,\n",
        "                    \"context_after\": context_after,\n",
        "                }\n",
        "            )\n",
        "    return candidates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gemini decision helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "\n",
        "def gemini_generate_text(prompt: str) -> str:\n",
        "    key = load_api_key()\n",
        "    if not os.environ.get(\"GEMINI_API_KEY\"):\n",
        "        os.environ[\"GEMINI_API_KEY\"] = key\n",
        "    client = genai.Client()\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-3-flash-preview\", contents=prompt\n",
        "    )\n",
        "    return response.text or \"\"\n",
        "\n",
        "\n",
        "def _format_context(items):\n",
        "    lines = []\n",
        "    for item in items:\n",
        "        start = f\"{float(item['start_sec']):.3f}\"\n",
        "        end = f\"{float(item['end_sec']):.3f}\"\n",
        "        text = str(item[\"text\"]).replace(\"\n",
        "\", \" \").strip()\n",
        "        lines.append(f\"{start}-{end} {text}\")\n",
        "    return lines\n",
        "\n",
        "\n",
        "def _build_prompt(candidates):\n",
        "    lines = []\n",
        "    lines.append(\n",
        "        \"Decide whether to CUT or KEEP each pause in a lecture video. \"\n",
        "        \"Keep pauses that add meaning (emphasis, transition, reflection). \"\n",
        "        \"Cut filler silence.\"\n",
        "    )\n",
        "    lines.append(\n",
        "        'Respond with JSON only: [{\"id\":\"...\",\"decision\":\"CUT|KEEP\",\"reason\":\"short\"}]'\n",
        "    )\n",
        "    lines.append(\"Candidates:\")\n",
        "    for cand in candidates:\n",
        "        lines.append(f\"ID: {cand['id']}\")\n",
        "        lines.append(\n",
        "            f\"gap_start: {cand['gap_start']:.3f}, gap_end: {cand['gap_end']:.3f}, \"\n",
        "            f\"gap_duration: {cand['gap_duration']:.3f}\"\n",
        "        )\n",
        "        before_lines = _format_context(cand[\"context_before\"])\n",
        "        after_lines = _format_context(cand[\"context_after\"])\n",
        "        lines.append(\"context_before:\")\n",
        "        lines.extend(before_lines or [\"(none)\"])\n",
        "        lines.append(\"context_after:\")\n",
        "        lines.extend(after_lines or [\"(none)\"])\n",
        "    return \"\n",
        "\".join(lines)\n",
        "\n",
        "\n",
        "def _extract_json(text: str):\n",
        "    text = text.strip()\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "    match = re.search(r\"(\\[.*\\]|\\{.*\\})\", text, re.S)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except json.JSONDecodeError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def _validate_response(payload, expected_ids):\n",
        "    if not isinstance(payload, list):\n",
        "        return None\n",
        "    by_id = {}\n",
        "    for item in payload:\n",
        "        if not isinstance(item, dict):\n",
        "            continue\n",
        "        gap_id = item.get(\"id\")\n",
        "        decision = item.get(\"decision\")\n",
        "        reason = item.get(\"reason\", \"\")\n",
        "        if gap_id in expected_ids and decision in (\"CUT\", \"KEEP\"):\n",
        "            by_id[gap_id] = {\"id\": gap_id, \"decision\": decision, \"reason\": str(reason)}\n",
        "    if set(by_id.keys()) != set(expected_ids):\n",
        "        return None\n",
        "    return list(by_id.values())\n",
        "\n",
        "\n",
        "def decide_gaps(candidates, batch_size: int = 10, max_retries: int = 2):\n",
        "    results = []\n",
        "    for start in range(0, len(candidates), batch_size):\n",
        "        batch = candidates[start : start + batch_size]\n",
        "        expected_ids = [item[\"id\"] for item in batch]\n",
        "        prompt = _build_prompt(batch)\n",
        "        attempt = 0\n",
        "        batch_result = None\n",
        "        while attempt <= max_retries:\n",
        "            response_text = gemini_generate_text(prompt)\n",
        "            payload = _extract_json(response_text)\n",
        "            batch_result = _validate_response(payload, expected_ids)\n",
        "            if batch_result is not None:\n",
        "                break\n",
        "            attempt += 1\n",
        "            time.sleep(0.5)\n",
        "        if batch_result is None:\n",
        "            raise RuntimeError(\"Gemini returned invalid JSON after retries.\")\n",
        "        results.extend(batch_result)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cut plan (keep segments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _merge_by_gap(segments: List[List[float]], gap_threshold: float):\n",
        "    if not segments:\n",
        "        return []\n",
        "    merged = [segments[0][:]]\n",
        "    for start, end in segments[1:]:\n",
        "        if start - merged[-1][1] <= gap_threshold:\n",
        "            merged[-1][1] = max(merged[-1][1], end)\n",
        "        else:\n",
        "            merged.append([start, end])\n",
        "    return merged\n",
        "\n",
        "\n",
        "def _enforce_min_length(segments: List[List[float]], min_len: float):\n",
        "    if not segments:\n",
        "        return []\n",
        "    i = 0\n",
        "    while i < len(segments):\n",
        "        start, end = segments[i]\n",
        "        if end - start >= min_len or len(segments) == 1:\n",
        "            i += 1\n",
        "            continue\n",
        "        if i == 0 and len(segments) > 1:\n",
        "            segments[1][0] = start\n",
        "            segments.pop(0)\n",
        "            continue\n",
        "        if i > 0:\n",
        "            segments[i - 1][1] = end\n",
        "            segments.pop(i)\n",
        "            continue\n",
        "        i += 1\n",
        "    return segments\n",
        "\n",
        "\n",
        "def compute_keep_segments(captions, candidates, decisions, merge_gap: float = 0.1, min_keep: float = 0.25):\n",
        "    if captions:\n",
        "        total_duration = max(float(item[\"end_sec\"]) for item in captions)\n",
        "    else:\n",
        "        total_duration = 0.0\n",
        "\n",
        "    decision_map = {item[\"id\"]: item for item in decisions}\n",
        "    segments = []\n",
        "    cursor = 0.0\n",
        "\n",
        "    for cand in candidates:\n",
        "        decision = decision_map.get(cand[\"id\"], {}).get(\"decision\", \"KEEP\")\n",
        "        if decision == \"CUT\":\n",
        "            if cand[\"gap_start\"] > cursor:\n",
        "                segments.append([cursor, cand[\"gap_start\"]])\n",
        "            cursor = max(cursor, cand[\"gap_end\"])\n",
        "\n",
        "    if total_duration > cursor:\n",
        "        segments.append([cursor, total_duration])\n",
        "\n",
        "    segments = _merge_by_gap(segments, merge_gap)\n",
        "    segments = _enforce_min_length(segments, min_keep)\n",
        "\n",
        "    return segments, total_duration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Render edited video with ffmpeg (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _has_audio(input_path: str) -> bool:\n",
        "    ffprobe = shutil.which(\"ffprobe\")\n",
        "    if not ffprobe:\n",
        "        return False\n",
        "    command = [\n",
        "        ffprobe,\n",
        "        \"-v\",\n",
        "        \"error\",\n",
        "        \"-select_streams\",\n",
        "        \"a\",\n",
        "        \"-show_entries\",\n",
        "        \"stream=index\",\n",
        "        \"-of\",\n",
        "        \"csv=p=0\",\n",
        "        input_path,\n",
        "    ]\n",
        "    result = subprocess.run(command, capture_output=True, text=True, check=False)\n",
        "    return bool(result.stdout.strip())\n",
        "\n",
        "\n",
        "def render_video(input_path: str, segments: List[List[float]], output_path: str):\n",
        "    ffmpeg = shutil.which(\"ffmpeg\")\n",
        "    if not ffmpeg:\n",
        "        raise RuntimeError(\"ffmpeg not found in PATH.\")\n",
        "    if not segments:\n",
        "        raise ValueError(\"No segments to render.\")\n",
        "\n",
        "    has_audio = _has_audio(input_path)\n",
        "    filter_parts = []\n",
        "    concat_inputs = []\n",
        "\n",
        "    for idx, (start, end) in enumerate(segments):\n",
        "        v_label = f\"v{idx}\"\n",
        "        filter_parts.append(\n",
        "            f\"[0:v]trim=start={start}:end={end},setpts=PTS-STARTPTS[{v_label}]\"\n",
        "        )\n",
        "        concat_inputs.append(f\"[{v_label}]\")\n",
        "        if has_audio:\n",
        "            a_label = f\"a{idx}\"\n",
        "            filter_parts.append(\n",
        "                f\"[0:a]atrim=start={start}:end={end},asetpts=PTS-STARTPTS[{a_label}]\"\n",
        "            )\n",
        "            concat_inputs.append(f\"[{a_label}]\")\n",
        "\n",
        "    if has_audio:\n",
        "        concat_filter = \"\".join(concat_inputs) + f\"concat=n={len(segments)}:v=1:a=1[v][a]\"\n",
        "        filter_parts.append(concat_filter)\n",
        "        command = [\n",
        "            ffmpeg,\n",
        "            \"-y\",\n",
        "            \"-i\",\n",
        "            input_path,\n",
        "            \"-filter_complex\",\n",
        "            \";\".join(filter_parts),\n",
        "            \"-map\",\n",
        "            \"[v]\",\n",
        "            \"-map\",\n",
        "            \"[a]\",\n",
        "            \"-movflags\",\n",
        "            \"+faststart\",\n",
        "            output_path,\n",
        "        ]\n",
        "    else:\n",
        "        concat_filter = \"\".join(concat_inputs) + f\"concat=n={len(segments)}:v=1:a=0[v]\"\n",
        "        filter_parts.append(concat_filter)\n",
        "        command = [\n",
        "            ffmpeg,\n",
        "            \"-y\",\n",
        "            \"-i\",\n",
        "            input_path,\n",
        "            \"-filter_complex\",\n",
        "            \";\".join(filter_parts),\n",
        "            \"-map\",\n",
        "            \"[v]\",\n",
        "            \"-movflags\",\n",
        "            \"+faststart\",\n",
        "            output_path,\n",
        "        ]\n",
        "\n",
        "    subprocess.run(command, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## End-to-end run helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_silence_cutter(video_path: str, transcript_path: str, outdir: str = \"outputs\", min_gap: float = 0.8, context: int = 2, batch_size: int = 10):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    captions = parse_transcript(transcript_path)\n",
        "    candidates = detect_gaps(captions, min_gap=min_gap, context=context)\n",
        "\n",
        "    decisions = []\n",
        "    if candidates:\n",
        "        decisions = decide_gaps(candidates, batch_size=batch_size, max_retries=2)\n",
        "\n",
        "    decisions_by_id = {item[\"id\"]: item for item in decisions}\n",
        "    for cand in candidates:\n",
        "        decision = decisions_by_id.get(cand[\"id\"], {\"decision\": \"KEEP\", \"reason\": \"\"})\n",
        "        cand[\"decision\"] = decision[\"decision\"]\n",
        "        cand[\"reason\"] = decision.get(\"reason\", \"\")\n",
        "\n",
        "    keep_segments, total_duration = compute_keep_segments(captions, candidates, decisions)\n",
        "    estimated_duration = sum(end - start for start, end in keep_segments)\n",
        "\n",
        "    cut_plan = {\n",
        "        \"video\": video_path,\n",
        "        \"transcript\": transcript_path,\n",
        "        \"min_gap\": min_gap,\n",
        "        \"context\": context,\n",
        "        \"total_duration_sec\": round(total_duration, 3),\n",
        "        \"estimated_edited_duration_sec\": round(estimated_duration, 3),\n",
        "        \"candidates\": candidates,\n",
        "        \"keep_segments\": [\n",
        "            {\n",
        "                \"start_sec\": round(start, 3),\n",
        "                \"end_sec\": round(end, 3),\n",
        "                \"duration_sec\": round(end - start, 3),\n",
        "            }\n",
        "            for start, end in keep_segments\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    cut_plan_path = os.path.join(outdir, \"cut_plan.json\")\n",
        "    keep_csv_path = os.path.join(outdir, \"keep_segments.csv\")\n",
        "    with open(cut_plan_path, \"w\", encoding=\"utf-8\") as handle:\n",
        "        json.dump(cut_plan, handle, indent=2)\n",
        "    with open(keep_csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as handle:\n",
        "        writer = csv.writer(handle)\n",
        "        writer.writerow([\"start_sec\", \"end_sec\", \"duration_sec\"])\n",
        "        for start, end in keep_segments:\n",
        "            writer.writerow([f\"{start:.3f}\", f\"{end:.3f}\", f\"{(end - start):.3f}\"])\n",
        "\n",
        "    edited_path = os.path.join(outdir, \"edited.mp4\")\n",
        "    render_error = None\n",
        "    if keep_segments:\n",
        "        try:\n",
        "            render_video(video_path, keep_segments, edited_path)\n",
        "        except Exception as exc:\n",
        "            render_error = str(exc)\n",
        "\n",
        "    summary = {\n",
        "        \"gaps_found\": len(candidates),\n",
        "        \"cut_count\": sum(1 for c in candidates if c.get(\"decision\") == \"CUT\"),\n",
        "        \"keep_count\": sum(1 for c in candidates if c.get(\"decision\") == \"KEEP\"),\n",
        "        \"total_duration_sec\": round(total_duration, 2),\n",
        "        \"estimated_duration_sec\": round(estimated_duration, 2),\n",
        "        \"edited_path\": edited_path if os.path.isfile(edited_path) else None,\n",
        "        \"render_error\": render_error,\n",
        "    }\n",
        "\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example:\n",
        "# summary = run_silence_cutter(\"path/to/video.mp4\", \"path/to/transcript.srt\", outdir=\"outputs\")\n",
        "# summary\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}